env_name: antmaze-medium-navigate-v0
reward_shaping: sparse
stochastic_xy: False
stochastic_sigma: 0.0
gid: 0
algo: higl
seed: 2
eval_freq: 5000.0
eval_episode_num: 10
max_timesteps: 1000000.0
step_update: False
step_update_interval: 50
no_correction: False
inner_dones: False
absolute_goal: False
binary_int_reward: False
man_tau: 0.005
man_batch_size: 128
man_buffer_size: 200000.0
man_rew_scale: 0.1
man_act_lr: 0.0001
man_crit_lr: 0.001
candidate_goals: 10
manager_propose_freq: 10
train_manager_freq: 10
discount: 0.99
sparse_rew_type: spa
correction_type: OPC
use_model_based_rollout: False
osp_delta: 20.0
osp_delta_update_rate: 0.0
rollout_exp_w: 0.95
fkm_hidden_size: 256
fkm_hidden_layer_num: 3
fkm_network_num: 5
fkm_batch_size: 512
fkm_lr: 0.005
fkm_obj_start_step: 20000
train_fkm_freq: 2000
ctrl_tau: 0.005
ctrl_batch_size: 128
ctrl_buffer_size: 200000.0
ctrl_rew_scale: 1.0
ctrl_act_lr: 0.0001
ctrl_crit_lr: 0.001
ctrl_discount: 0.95
ctrl_mgp_lambda: 1.0
ctrl_osrp_lambda: 0.0005
ctrl_gcmr_start_step: 20000
noise_type: normal
ctrl_noise_sigma: 1.0
man_noise_sigma: 1.0
train_ctrl_policy_noise: 0.2
train_ctrl_noise_clip: 0.5
train_man_policy_noise: 0.2
train_man_noise_clip: 0.5
traj_buffer_size: 50000
lr_r: 0.0002
r_margin_pos: 1.0
r_margin_neg: 1.2
r_training_epochs: 25
r_batch_size: 64
r_hidden_dim: 128
r_embedding_dim: 32
goal_loss_coeff: 20.0
landmark_loss_coeff: 1.0
delta: 2.0
adj_factor: 0.5
landmark_sampling: fps
clip_v: -38.0
n_landmark_coverage: 60
initial_sample: 1000
goal_thr: -10.0
planner_start_step: 60000
novelty_algo: rnd
use_novelty_landmark: True
close_thr: 0.2
n_landmark_novelty: 60
rnd_output_dim: 128
rnd_lr: 0.001
rnd_batch_size: 128
use_ag_as_input: False
no_pseudo_landmark: False
discard_by_anet: False
automatic_delta_pseudo: False
save_models: False
save_dir: ./models
save_replay_buffer: False
load: False
load_dir: ./models/steps/450000
load_algo: higl
log_dir: /home/yuanwenyu/DCA-HRL_explore/logs
load_replay_buffer: False
load_adj_net: False
version: sparse_dca
cov_rew_scale: 0.1
ent_rew_scale: 0.001
count_bonus_coef: 0.001
relara_beta: 0.01
ra_policy_frequency: 2
